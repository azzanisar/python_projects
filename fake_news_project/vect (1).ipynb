{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from random import randint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from newspaper import Article\n",
    "from xgboost import XGBClassifier   \n",
    "import torch\n",
    "import openai\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from joblib import dump\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract and clean text from a URL\n",
    "def extract_text_clean(url):\n",
    "    \n",
    "    # Make a request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the text from the HTML content\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Remove specific words\n",
    "    remove_words = ['newsbbc', 'homepageskip', 'contentaccessibility', 'helpyour', 'accounthomenewssportreelworklifetravelfuturemore', 'menumore', 'menusearch', 'bbchomenewssportreelworklifetravelfutureculturemusictvweathersoundsclose', 'menubbc', 'newsmenuhomewar', 'ukraineclimatevideoworldukbusinesstechsciencestoriesmoreentertainment', 'artshealthworld', 'news', 'tvin', 'picturesreality', 'checknewsbeatlong', 'readsworldafricaasiaaustraliaeuropelatin', 'americamiddle', 'eastus']\n",
    "    tokens = [token for token in tokens if token not in remove_words]\n",
    "\n",
    "    # Join the tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Define a function to extract and clean text from a URL\n",
    "def clean_text(text):\n",
    "      \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ensemble_pickle', 'rb') as f:\n",
    "    loaded_model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vec2_pickle', 'rb') as f:\n",
    "    vec=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART model and tokenizer for summarization\n",
    "model_summarization = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer_summarization = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "def summarize_article(article_text):\n",
    "    # Tokenize the article\n",
    "    input_ids = tokenizer_summarization(article_text, max_length=1024, truncation=True, return_tensors='pt').input_ids\n",
    "    \n",
    "    # Generate a summary of the article\n",
    "    summary_ids = model_summarization.generate(input_ids, num_beams=4, max_length=500, early_stopping=True)\n",
    "    \n",
    "    # Decode the summary text\n",
    "    summary_text = tokenizer_summarization.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "# Test the function on a sample URL\n",
    "url = 'https://www.bbc.com/news/world-us-canada-55568621'\n",
    "article_text = extract_text_clean(url)\n",
    "summ_article = summarize_article(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_article_ensemble(url):\n",
    "    \n",
    "    #get the article and summarize it using bert\n",
    "    cleaned_text = extract_text_clean(url)\n",
    "    summ_article = summarize_article(cleaned_text)\n",
    "    \n",
    "    # Set OpenAI API key\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    #get gpt vote\n",
    "    cont = f'{summ_article} do fact checking and provide 1 source at the end'\n",
    "    completion = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=[{'role':'user', 'content':cont}])\n",
    "    gpt_answer = completion.choices[0].message.content\n",
    "    \n",
    "    # Vectorize the text\n",
    "    vectorized_text = vec.transform([cleaned_text])\n",
    "    \n",
    "    # Predict the class label prediction probability of the input text using the loaded model\n",
    "    class_label = loaded_model.predict_proba(vectorized_text)[0][0]\n",
    "    \n",
    "    print(f\"Estimated Fake news probability: {class_label*100:.2f}% \\n {gpt_answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Fake news probability: 72.43% \n",
      " Fact check:\n",
      "\n",
      "The third child of the Duke and Duchess of Cambridge was born on April 23, 2018, at St. Mary's Hospital in London. He weighed 8lbs 7oz and is fifth in line to the throne. His elder siblings are Prince George and Princess Charlotte.\n",
      "\n",
      "Source: \n",
      "BBC News. (2018, April 23). Royal baby: Duke and duchess show off new son. Retrieved from https://www.bbc.com/news/uk-43864933\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "url = 'https://www.bbc.com/news/uk-43864933'\n",
    "prediction = predict_article_ensemble(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
