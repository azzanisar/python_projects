# -*- coding: utf-8 -*-
"""TextSummarizationPage.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BVeH3liYPoD6cOmzqBsRJJoukAQvSfUF
"""

#!pip install -q streamlit

#pip install contractions cleantext

import contractions
import matplotlib.pyplot as plt
import string
import re
from cleantext import clean
import random
from bs4 import BeautifulSoup
import requests
import streamlit as st
from streamlit_lottie import st_lottie
#import Text_Summarizer
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from collections import Counter
from heapq import nlargest

st.set_page_config (
    page_title="Text Summarization",
    page_icon="ðŸ“±",
)
st.title("Text Summarization")

def load_summarise(popup):
     r=requests.get(popup)
     if r.status_code != 200:
         return None
     return r.json()

lottie_summarise= load_summarise("https://assets7.lottiefiles.com/packages/lf20_q7ia4fyk.json")
with st.container():
    st.write('---')
    left_column, right_column=st.columns(2)
    with left_column:
        st.header("How can we summarise our text?")
    with right_column:
        st_lottie(lottie_summarise, height=300, key="news")


class Preprocessing_token():
    def __init__(self,token):
        self.token = token

    # returns the expanded version of contractions
    def remove_contractions(self, token):
        token = contractions.fix(token.lower())
        return token

    #convert all words to lower case
    def remove_uppercase(self, token):
        token = token.lower()
        return token

    #Remove Punctuation
    def remove_punctuation(self, token):
        token =  re.sub('[%s]' % re.escape(string.punctuation), '' , token)
        return token

    #Remove Numbers
    def remove_numbers(self, token):
        token = re.sub(r'\d+', '', token)
        return token

    #Remove whitespace
    def remove_whitespace(self, token):
        token = " ".join(token.split()) #split text then join with space between words
        return token

    #remove Emojis
    def remove_emojis(self, token):
        regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
        return regrex_pattern.sub(r'',token)

    #remove html tags based on given filter tags (it basically gets the content inside given filter tags)
    def remove_html(self,html,tags):
      #using html parser to sort out text only
      soup = BeautifulSoup(html, 'html.parser')
      #scraping only title and paragraph
      results = soup.find_all(tags)
      #saving the results generated
      text = [result.text for result in results]
      ARTICLE = ' '.join(text)
      return ARTICLE

# Regular expression pattern to validate URL
url_pattern = re.compile(
    r'^(?:http|ftp)s?://'  # http:// or https://
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|'  # domain...
    r'localhost|'  # localhost...
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|'  # ...or IPv4
    r'\[?[A-F0-9]*:[A-F0-9:]+\]?)'  # ...or IPv6
    r'(?::\d+)?'  # optional port
    r'(?:/?|[/?]\S+)$', re.IGNORECASE)

def summarising(doc):
  nlp = spacy.load('en_core_web_sm')
  preprocessor = Preprocessing_token("")
  doc = preprocessor.remove_uppercase(doc)
  doc = preprocessor.remove_punctuation(doc)
  doc = preprocessor.remove_numbers(doc)
  doc = preprocessor.remove_whitespace(doc)
  doc = preprocessor.remove_emojis(doc)
  doc = preprocessor.remove_contractions(doc)
  doc = nlp(doc)

  keyword = []
  stopwords = list(STOP_WORDS)
  pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']
  for token in doc:
      if(token.text in stopwords or token.text in punctuation):
          continue
      if(token.pos_ in pos_tag):
          keyword.append(token.text)
  freq_word = Counter(keyword)
  max_freq=Counter(keyword).most_common(1)[0][1]
  print(max_freq)
  for word in freq_word.keys():
          freq_word[word] = (freq_word[word]/max_freq)
  sent_strength={}
  for sent in doc.sents:
      for word in sent:
          if word.text in freq_word.keys():
              if sent in sent_strength.keys():
                  sent_strength[sent]+=freq_word[word.text]
              else:
                  sent_strength[sent]=freq_word[word.text]

  #Set the output summarized sentence count
  summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)


  final_sentences = [ w.text for w in summarized_sentences ]
  summary = ' '.join(final_sentences)

  return summary
# Get the URL input from the user
URL = st.text_input("Enter URL")

# Validate the URL input
if URL and not re.match(url_pattern, URL):
    st.warning("Invalid URL format. Please enter a valid URL.")
else:
    # Use the URL input in your application logic
    st.write("URL entered:", URL)

#URL = "https://www.sondakika.com/politika/haber-cumhurbaskani-erdogan-eve-donus-projeleri-ile-6-bin-bilim-insanimizi-geri-kazandirdik-15822485/"
if st.button("Process"):
      if URL:
          r = requests.get(URL)
          pre = Preprocessing_token("")
          text = pre.remove_html(r.text,["h1","p"])
          st.write("Summarising:" , summarising(text))
      else:
            st.warning("Please enter a URL.")

text_box = st.text_area("Paste your text here")
button_doc=st.button("Click to Enter")
if button_doc:
     st.write(summarising(text_box))
